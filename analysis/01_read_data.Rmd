---
title: "01_read_data"
author: "Ross Gayler"
date: "2023-01-12"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r setup, echo = FALSE,	message = FALSE, warning = FALSE}
library(here) # relative file locations
library(fs) # file system operations
library(tibble) # enhanced data frames
library(dplyr) # data manipulation
library(stringr) # string manipulation
library(readr) # read CSV files
library(tidyr) # tidy messy data
library(purrr) # functional programming tools
library(gt) # table formatting
```

## Introduction

Read the raw data from all the data files, reshape it into a more useful format, save it as an R object, and provide a summary of the contents to give some assurance that the data was read correctly.

### Data organisation

The data are generated by applying models to some of the test datasets from the [UCR Time Series Classification Repository](https://www.timeseriesclassification.com/index.php).
All the models are classification models, that is, they assign each case to one of a fixed set of dataset-specific classes.

All the models of interest here map each case to a vector of scores, one for each class.
The case is categorised as belonging to the class with the highest score.

All the raw data are stored in `data/UCR_Data_Scores`.

There is a separate subdirectory of `data/UCR_Data_Scores` for each dataset analysed (e.g. `data/UCR_Data_Scores/UCR_14`).

The datasets used are shown in the table below.
(The table has to be manually populated.)

| Dataset | Name            | Description page                                                                   |
|------------------|------------------|-----------------------------------|
| UCR_14  | CinCECGTorso    | <https://www.timeseriesclassification.com/description.php?Dataset=CinCECGTorso>    |
| UCR_48  | GestureMidAirD3 | <https://www.timeseriesclassification.com/description.php?Dataset=GestureMidAirD3> |

: Table of dataset names and links to their online descriptions.

In the subdirectory for each dataset analysed there are multiple files, each corresponding to the application of a single model to that dataset.
The file naming convention is `ModelName_test_results.csv` (e.g. `MINIROCKET_test_results.csv`).

Each data file is a CSV with $k + 1$ columns, where $k$ is the number of classes.

Each row corresponds to a case from the UCR dataset that has been processed by the model corresponding to the file.

The first column contains an integer in the range $[0, k - 1]$, indicating which of the $k$ classes is the "true" class of the case.

The remaining $k$ columns are the class scores for the case for each of the classes in order from $0$ to $k - 1$.

### Output organisation

The data from all the UCR datasets is concatenated into a single R data frame and saved as an RDS file (`output/d_scores.RDS`).

Different datasets have different numbers of classes, so the data is pivoted from wide to tall format to enable concatenation of the datasets into a single data frame.

The columns of the data frame are: `dataset`, `model`, `class_id`, `score_id`, `score_val`.

## Get the file names

Get the names of all the data files and extract the dataset and model names.

```{r}
d_files <- here::here("data/UCR_Data_Scores") |>
  fs::dir_ls(glob = "*_test_results.csv", recurse = 1) |>
  tibble::as_tibble_col(column_name = "path") |>
  dplyr::arrange(path) |>
  dplyr::mutate(
    dataset = fs::path_dir(path) |>
      stringr::str_remove(pattern = ".*/"),
    model = fs::path_file(path) |>
      stringr::str_remove(pattern = "_test_results\\.csv"),
  )

# quick view of the data files to be read
d_files
```

## Read the files

Create a function to read one file and reformat it.

```{r}
read_1 <- function(
    path, # character - path of file to read
    dataset, # character - ID of dataset
    model # character - name of model applied to dataset
) {
  # read the file
  d <- readr::read_csv(path, col_names = FALSE, show_col_types = FALSE)
  
  # rename the columns
  n_class <- ncol(d) - 1 # 1 column for each class score plus 1 for the true class
  colnames(d) <- c("class_id", paste0("score_", 0:(n_class - 1))) # 0-origin class indexing
  
  d |>
    # add file identifiers and within-file case numbers
    dplyr::mutate(
      dataset = dataset,
      model = model,
      case = 1:n(),
      # force the types for neatness
      class_id = as.integer(class_id)
    ) |>
    # reformat to long
    tidyr::pivot_longer( 
      cols = tidyr::starts_with("score_"), 
      names_to = "score_id",
      names_prefix = "score_",
      values_to = "score_val"
    ) |>
    dplyr::mutate(
      # force the types for neatness
      score_id = as.integer(score_id)
    ) |>
    # reorder the columns for more intuitive display
    dplyr::relocate(dataset, model, case)
}
```

Read all the files and concatenate them.

```{r}
d_scores <- purrr::pmap_dfr(.l = d_files, .f = read_1)

# quick view of the data that was read
d_scores
```

Save the concatenated data.

```{r}
d_scores |> saveRDS(file = here::here("output", "d_scores.RDS"))
```

## Get check summaries

Calculate the number of observations, classes, and scores per file as a basic check.
These need to be manually checked against the metadata for the datasets.

```{r}
d_scores |>
  dplyr::group_by(dataset, model) |>
  dplyr::summarise(
    n_case = max(case),
    min_class_id = min(class_id),
    max_class_id = max(class_id),
    n_class_id = unique(class_id) |> length(),
    min_score_id = min(score_id),
    max_score_id = max(score_id),
    n_score_id = unique(score_id) |> length()
  ) |>
  gt::gt()
```

*That looks as expected.*

Calculate the number of observations and scores for each class in each file.
These need to be manually checked against the metadata for the datasets.

```{r}
d_scores |>
  dplyr::group_by(dataset, model, class_id) |>
  dplyr::summarise(
    n_case = unique(case) |> length(),
    n_score = unique(score_id) |> length()
  ) |>
  gt::gt()
```

*That looks as expected.*
